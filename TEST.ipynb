{"cells":[{"cell_type":"markdown","id":"6394d198","metadata":{"id":"6394d198"},"source":["## Import Libraries :"]},{"cell_type":"code","execution_count":null,"id":"2f42b9a4","metadata":{"id":"2f42b9a4"},"outputs":[],"source":["import numpy as np\n","from movenet import Movenet\n","from data import BodyPart\n","import cv2\n","\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import load_model"]},{"cell_type":"markdown","id":"d04d2d29","metadata":{"id":"d04d2d29"},"source":["## Data path :"]},{"cell_type":"code","execution_count":null,"id":"d4edf4aa","metadata":{"id":"d4edf4aa"},"outputs":[],"source":["ROOT = 'C:/Users/admin/miniconda_codes/POSE/'\n","movenet_model_path = ROOT + 'models/movenet_thunder'\n","pose_yoga_model_path = ROOT + 'models/my_model'\n","\n","Yoga_pose_imgs_path =  ROOT + 'Pose_imgs/'\n","Yoga_imgs_path =  ROOT + 'imgs/'\n","Yoga_videos_path = ROOT + 'Videos/'"]},{"cell_type":"markdown","id":"96b76daf","metadata":{"id":"96b76daf"},"source":["## Load Models :"]},{"cell_type":"code","execution_count":null,"id":"e9f3b741","metadata":{"id":"e9f3b741"},"outputs":[],"source":["movenet = Movenet(movenet_model_path)\n","model = load_model(pose_yoga_model_path)"]},{"cell_type":"code","execution_count":null,"id":"38b2b6b2","metadata":{"id":"38b2b6b2","outputId":"676f549b-b213-4bb3-830a-6910586957cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 34)]              0         \n","                                                                 \n"," dense (Dense)               (None, 128)               4480      \n","                                                                 \n"," dropout (Dropout)           (None, 128)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 64)                8256      \n","                                                                 \n"," dropout_1 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 8)                 520       \n","                                                                 \n","=================================================================\n","Total params: 13,256\n","Trainable params: 13,256\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","id":"da31d513","metadata":{"id":"da31d513"},"source":["## Variables :"]},{"cell_type":"code","execution_count":null,"id":"20ff3ac4","metadata":{"id":"20ff3ac4"},"outputs":[],"source":["pose_classes = ['chair', 'cobra', 'dog', \n","                'no_pose', 'shoudler_stand', 'print',\n","                'tree', 'warrior']\n","\n","colors = [(255,255,255), (0,0,255)]\n","\n","detection_threshold=0.1\n","\n","EDGES = {\n","    (0, 1): 'm',\n","    (0, 2): 'c',\n","    (1, 3): 'm',\n","    (2, 4): 'c',\n","    (0, 5): 'm',\n","    (0, 6): 'c',\n","    (5, 7): 'm',\n","    (7, 9): 'm',\n","    (6, 8): 'c',\n","    (8, 10): 'c',\n","    (5, 6): 'y',\n","    (5, 11): 'm',\n","    (6, 12): 'c',\n","    (11, 12): 'y',\n","    (11, 13): 'm',\n","    (13, 15): 'm',\n","    (12, 14): 'c',\n","    (14, 16): 'c'\n","}"]},{"cell_type":"markdown","id":"9e3fce18","metadata":{"id":"9e3fce18"},"source":["## Movenet_functions :"]},{"cell_type":"code","execution_count":null,"id":"13eb43a7","metadata":{"id":"13eb43a7"},"outputs":[],"source":["def img_to_tensor(im):\n","    im = tf.convert_to_tensor(im, dtype=tf.uint8)\n","    return im"]},{"cell_type":"code","execution_count":null,"id":"55c2cc90","metadata":{"id":"55c2cc90"},"outputs":[],"source":["def detect(input_tensor, inference_count=3):\n","    movenet.detect(input_tensor.numpy(), reset_crop_region=True)\n","    \n","    for _ in range(inference_count - 1):\n","        detection = movenet.detect(input_tensor.numpy(), \n","                                reset_crop_region=False)\n","    \n","    return detection"]},{"cell_type":"code","execution_count":null,"id":"0f8be5d6","metadata":{"id":"0f8be5d6"},"outputs":[],"source":["def draw_connection(frame, keypoints, edges, color, confidence_threshold = 0.1, thickness = 1):\n","    \n","    for edge, _ in edges.items():\n","        p1, p2 = edge\n","        y1, x1, c1 = keypoints[p1]\n","        y2, x2, c2 = keypoints[p2]\n","        \n","        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n","            cv2.line(frame, (int(y1), int(x1)), (int(y2), int(x2)), color, thickness)"]},{"cell_type":"code","execution_count":null,"id":"ac0347c2","metadata":{"id":"ac0347c2"},"outputs":[],"source":["def draw_keypoints(frame, keypoints, color, confidence_threshold = 0.1, thickness = 3, fill_inside = 1):\n","  \n","    for kp in keypoints:\n","        ky, kx, kp_conf = kp       \n","        if kp_conf > confidence_threshold:\n","            cv2.circle(frame, (int(ky), int(kx)), thickness, color, fill_inside)\n"]},{"cell_type":"markdown","id":"9db6f64a","metadata":{"id":"9db6f64a"},"source":["## Test movenet with imgs :"]},{"cell_type":"code","execution_count":null,"id":"e76b6ff1","metadata":{"id":"e76b6ff1"},"outputs":[],"source":["img_path = Yoga_imgs_path +'2.jpg'\n","img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n","cv2.imshow('Input', img)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"id":"50fabca8","metadata":{"id":"50fabca8"},"outputs":[],"source":["input_img = img_to_tensor(img)\n","keypoints_with_scores = detect(input_img)\n","\n","min_landmark_score = min([keypoint.score for keypoint in keypoints_with_scores.keypoints])\n","should_keep_image = min_landmark_score >= detection_threshold\n","if not should_keep_image:\n","    self._message.append('Skipped' + image_path + 'Keypoints score are below than threshold')\n","pose_landmarks = [[int(keypoint.coordinate.x), int(keypoint.coordinate.y), keypoint.score] for keypoint in keypoints_with_scores.keypoints]\n","draw_keypoints(img, pose_landmarks,  colors[0])\n","draw_connection(img, pose_landmarks, EDGES, colors[1]) \n","cv2.imshow('Output', img)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","id":"35402ac4","metadata":{"id":"35402ac4"},"source":["## Test movenet with videos :"]},{"cell_type":"code","execution_count":null,"id":"84c8614c","metadata":{"id":"84c8614c"},"outputs":[],"source":["file = Yoga_videos_path + 'v1.mp4'\n","cap = cv2.VideoCapture(file)\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    \n","    # Detect pose keypoints:\n","    input_img = img_to_tensor(frame)\n","    keypoints_with_scores = detect(input_img, 2)\n","    \n","    pose_landmarks = [[int(keypoint.coordinate.x), int(keypoint.coordinate.y), keypoint.score] for keypoint in keypoints_with_scores.keypoints] \n","    \n","    draw_keypoints(frame, pose_landmarks,  colors[0])\n","    draw_connection(frame, pose_landmarks, EDGES, colors[1])    \n","    \n","    \n","    cv2.imshow('Yoga_Poses', frame)\n","        \n","    if cv2.waitKey(10) & 0xFF==ord('q'):\n","        break\n","cap.release()\n","cv2.destroyAllWindows() "]},{"cell_type":"markdown","id":"f3cf7618","metadata":{"id":"f3cf7618"},"source":["## Model Functions :"]},{"cell_type":"code","execution_count":null,"id":"1b1bb446","metadata":{"id":"1b1bb446"},"outputs":[],"source":["def get_center_point(landmarks, left_bodypart, right_bodypart):\n","    \"\"\"Calculates the center point of the two given landmarks.\"\"\"\n","    left = tf.gather(landmarks, left_bodypart.value, axis=1)\n","    right = tf.gather(landmarks, right_bodypart.value, axis=1)\n","    center = left * 0.5 + right * 0.5\n","    return center\n","\n","\n","def get_pose_size(landmarks, torso_size_multiplier=2.5):\n","    \"\"\"Calculates pose size.\n","    It is the maximum of two values:\n","    * Torso size multiplied by `torso_size_multiplier`\n","    * Maximum distance from pose center to any pose landmark\n","    \"\"\"\n","    # Hips center\n","    hips_center = get_center_point(landmarks, BodyPart.LEFT_HIP, \n","                                 BodyPart.RIGHT_HIP)\n","\n","    # Shoulders center\n","    shoulders_center = get_center_point(landmarks, BodyPart.LEFT_SHOULDER,\n","                                      BodyPart.RIGHT_SHOULDER)\n","\n","    # Torso size as the minimum body size\n","    torso_size = tf.linalg.norm(shoulders_center - hips_center)\n","    # Pose center\n","    pose_center_new = get_center_point(landmarks, BodyPart.LEFT_HIP, \n","                                     BodyPart.RIGHT_HIP)\n","    pose_center_new = tf.expand_dims(pose_center_new, axis=1)\n","    # Broadcast the pose center to the same size as the landmark vector to\n","    # perform substraction\n","    pose_center_new = tf.broadcast_to(pose_center_new,\n","                                    [tf.size(landmarks) // (17*2), 17, 2])\n","\n","    # Dist to pose center\n","    d = tf.gather(landmarks - pose_center_new, 0, axis=0,\n","                name=\"dist_to_pose_center\")\n","    # Max dist to pose center\n","    max_dist = tf.reduce_max(tf.linalg.norm(d, axis=0))\n","\n","    # Normalize scale\n","    pose_size = tf.maximum(torso_size * torso_size_multiplier, max_dist)\n","    return pose_size\n","\n","\n","\n","def normalize_pose_landmarks(landmarks):\n","    \"\"\"Normalizes the landmarks translation by moving the pose center to (0,0) and\n","    scaling it to a constant pose size.\n","  \"\"\"\n","  # Move landmarks so that the pose center becomes (0,0)\n","    pose_center = get_center_point(landmarks, BodyPart.LEFT_HIP, \n","                                 BodyPart.RIGHT_HIP)\n","\n","    pose_center = tf.expand_dims(pose_center, axis=1)\n","    # Broadcast the pose center to the same size as the landmark vector to perform\n","    # substraction\n","    pose_center = tf.broadcast_to(pose_center, \n","                                [tf.size(landmarks) // (17*2), 17, 2])\n","    landmarks = landmarks - pose_center\n","\n","    # Scale the landmarks to a constant pose size\n","    pose_size = get_pose_size(landmarks)\n","    landmarks /= pose_size\n","    return landmarks\n","\n","\n","def landmarks_to_embedding(landmarks_and_scores):\n","    \"\"\"Converts the input landmarks into a pose embedding.\"\"\"\n","    # Reshape the flat input into a matrix with shape=(17, 3)\n","    reshaped_inputs = keras.layers.Reshape((17, 3))(landmarks_and_scores)\n","\n","    # Normalize landmarks 2D\n","    landmarks = normalize_pose_landmarks(reshaped_inputs[:, :, :2])\n","    # Flatten the normalized landmark coordinates into a vector\n","    embedding = keras.layers.Flatten()(landmarks)\n","    return embedding\n","\n","\n","def preprocess_data(X_test):\n","    embedding = landmarks_to_embedding(tf.reshape(tf.convert_to_tensor(X_test), (1, 51)))\n","    processed_X_test = (tf.reshape(embedding, (34)))\n","    processed_X_test = tf.convert_to_tensor(processed_X_test)\n","    processed_X_test = tf.expand_dims(processed_X_test, axis=0)\n","    return processed_X_test\n"]},{"cell_type":"code","execution_count":null,"id":"629c3976","metadata":{"id":"629c3976"},"outputs":[],"source":["def overlap_a_b(a, b):\n","    alpha = 0.5\n","    beta = 0.5\n","    gamma = 0\n","    out_img = cv2.addWeighted(a, alpha, b, beta, gamma)\n","    \n","    return out_img"]},{"cell_type":"code","execution_count":null,"id":"3db34ec3","metadata":{"id":"3db34ec3"},"outputs":[],"source":["def process_model(frame, yoga_pose_img, class_id):\n","    yoga_x, yoga_y, _ = yoga_pose_img.shape\n","    # Detect pose keypoints:\n","    input_img = img_to_tensor(frame)\n","    keypoints_with_scores = detect(input_img)\n","    \n","    \n","    pose_landmarks = [[int(keypoint.coordinate.x), int(keypoint.coordinate.y), keypoint.score] for keypoint in keypoints_with_scores.keypoints]\n","    input_landmarks = np.array(pose_landmarks,dtype=np.float32).flatten()\n","    model_input = preprocess_data(input_landmarks)\n","    result = model.predict(model_input)\n","    \n","    overlap_imgs = overlap_a_b(frame[50:yoga_y+50,0:yoga_x], yoga_pose_img)\n","    frame[50:yoga_y+50,0:yoga_x] = overlap_imgs\n","    \n","    if (np.argmax(result)) == class_id:\n","        \n","        draw_keypoints(frame, pose_landmarks,  colors[0])\n","        draw_connection(frame, pose_landmarks, EDGES, colors[0])  \n","        \n","        \n","\n","    \n","    else: \n","        draw_keypoints(frame, pose_landmarks,  colors[1])\n","        draw_connection(frame, pose_landmarks, EDGES, colors[1]) \n","        \n","    cv2.putText(frame, \n","                ('Pose: {} with a {:.2f} %. '.format(pose_classes[np.argmax(result)], 100 * np.max(result))), \n","                (5, 25), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)"]},{"cell_type":"markdown","id":"a0715123","metadata":{"id":"a0715123"},"source":["## Model Test with imgs :\n","\n","\n","pose_classes = ['chair', 'cobra', 'dog', \n","                'no_pose', 'shoudler_stand', 'print',\n","                'tree', 'warrior']"]},{"cell_type":"code","execution_count":null,"id":"7d970512","metadata":{"id":"7d970512"},"outputs":[],"source":["img_path = Yoga_imgs_path +'1.jpg'\n","img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n","cv2.imshow('Input', img)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"id":"138d0e7d","metadata":{"id":"138d0e7d","outputId":"1f92cb71-7cd4-4697-db8d-ff1a46cd636f"},"outputs":[{"name":"stdout","output_type":"stream","text":["This image most likely belongs to tree with a 100.00 percent confidence.\n"]}],"source":["process_model(img, class_id=0) \n","cv2.imshow('Output', img)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","id":"7b92b795","metadata":{"id":"7b92b795"},"source":["## Model Test with videos :"]},{"cell_type":"code","execution_count":null,"id":"d4aa149f","metadata":{"id":"d4aa149f","outputId":"786fe4a6-42be-4798-9c0e-17f51c39b48c"},"outputs":[{"name":"stdout","output_type":"stream","text":["30\n"]}],"source":["Yoga_pose_path = Yoga_pose_imgs_path +'chair_3.jpg'\n","Yoga_pose = cv2.imread(Yoga_pose_path, cv2.IMREAD_COLOR)\n","Yoga_pose = cv2.resize(Yoga_pose, (120,120), interpolation = cv2.INTER_AREA)\n","\n","file = Yoga_videos_path + 'vid4.mp4'\n","cap = cv2.VideoCapture(file)\n","\n","video_width = int(cap.get(3))\n","video_height = int(cap.get(4))\n","FPS = int(cap.get(cv2.CAP_PROP_FPS))\n","print(FPS)\n","video_size = (int(video_width/2), int(video_height/2))\n","\n","\n","filename = (Yoga_videos_path + 'testing_vid.avi')\n","out_video = cv2.VideoWriter(filename, cv2.VideoWriter_fourcc(*'MJPG'), FPS, video_size)\n","\n","\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    \n","    try:\n","    \n","        frame = cv2.resize(frame, video_size, interpolation = cv2.INTER_AREA)\n","\n","        process_model(frame, Yoga_pose, class_id=0)\n","\n","\n","        out_video.write(frame)\n","        cv2.imshow('Yoga_Poses', frame)\n","\n","        if cv2.waitKey(10) & 0xFF==ord('q'):\n","            break\n","    except: pass\n","        \n","cap.release()\n","out_video.release()\n","cv2.destroyAllWindows() "]},{"cell_type":"code","execution_count":null,"id":"3a2cbc8d","metadata":{"id":"3a2cbc8d","outputId":"0ebbd468-5386-4529-a34b-33af947c3ce9"},"outputs":[{"data":{"text/plain":["(1264, 824)"]},"execution_count":174,"metadata":{},"output_type":"execute_result"}],"source":["video_size"]},{"cell_type":"code","execution_count":null,"id":"6afc8e49","metadata":{"id":"6afc8e49"},"outputs":[],"source":["img_path = Yoga_pose_imgs_path +'chair.png'\n","test = cv2.imread(img_path, cv2.IMREAD_COLOR)\n","test = cv2.resize(test, (60,60), interpolation = cv2.INTER_AREA)\n","cv2.imshow('Input', test)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"id":"17c6060b","metadata":{"id":"17c6060b","outputId":"1ab75ebc-7298-4359-fe41-f5597df06c4f"},"outputs":[{"data":{"text/plain":["(60, 60, 3)"]},"execution_count":131,"metadata":{},"output_type":"execute_result"}],"source":["test.shape"]},{"cell_type":"code","execution_count":null,"id":"092c9353","metadata":{"id":"092c9353"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"ef215caf","metadata":{"id":"ef215caf"},"outputs":[],"source":["cv2.imshow('overlap', img)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"id":"36b8240f","metadata":{"id":"36b8240f"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"TEST.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}